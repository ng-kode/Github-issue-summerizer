{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/deep_learn_p3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for tk_body.dpkl: 8000\n",
      "Size of vocabulary for tk_title.dpkl: 4500\n"
     ]
    }
   ],
   "source": [
    "from Helpers import load_tokenizer\n",
    "\n",
    "tk_body = load_tokenizer('tk_body.dpkl')\n",
    "tk_title = load_tokenizer('tk_title.dpkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Current best 'seq2seq_model_bi1.h5\n",
    "seq2seq_Model = load_model('seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract encoder; Re-structure decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = seq2seq_Model.get_layer('Encoder-Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, merge\n",
    "from keras.models import Model\n",
    "\n",
    "def extract_decoder_model(model):\n",
    "    latent_dim = model.get_layer('Title-Word-Embedding').output_shape[-1]\n",
    "    \n",
    "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
    "    dec_emb = model.get_layer('Title-Word-Embedding')(decoder_inputs)\n",
    "    dec_bn = model.get_layer('Decoder-BatchNorm-1')(dec_emb)\n",
    "    \n",
    "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "    \n",
    "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input]) \n",
    "#     gru_out_back, gru_state_out_back = model.get_layer('Decoder-Backward-GRU')([dec_bn, gru_inference_state_input]) \n",
    "    \n",
    "#     gru_out = merge([gru_out, gru_out_back], mode='concat')\n",
    "    \n",
    "    dec_bn2 = model.get_layer('Decoder-BatchNorm-2')(gru_out)\n",
    "    \n",
    "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
    "    \n",
    "    decoder_model = Model([decoder_inputs, gru_inference_state_input],\n",
    "                          [dense_out, gru_state_out])\n",
    "    return decoder_model\n",
    "\n",
    "decoder_model = extract_decoder_model(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue Body:\n",
      " \n",
      "I am trying to implement a LSTM based speech recognizer. So far I could set up bidirectional LSTM (i think it is working as a bidirectional LSTM) by following the example in Merge layer. Now I want to try it with another bidirectional LSTM layer, which make it a deep bidirectional LSTM. But I am unable to figure out how to connect the output of the previously merged two layers into a second set of LSTM layers. I don't know whether it is possible with Keras. Hope someone can help me with this.\n",
      " \n",
      "\n",
      "\n",
      ">>>>> Generated Title (Prediction): <<<<<\n",
      " lstm layer\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Helpers import create_title\n",
    "\n",
    "body_text = \"\"\"\n",
    "I am trying to implement a LSTM based speech recognizer. So far I could set up bidirectional LSTM (i think it is working as a bidirectional LSTM) by following the example in Merge layer. Now I want to try it with another bidirectional LSTM layer, which make it a deep bidirectional LSTM. But I am unable to figure out how to connect the output of the previously merged two layers into a second set of LSTM layers. I don't know whether it is possible with Keras. Hope someone can help me with this.\n",
    "\"\"\"\n",
    "\n",
    "create_title(body_text, tk_body, tk_title, encoder_model, decoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See some github issues examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('github_issues.csv').sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_text = df.body.tolist()\n",
    "title_text = df.issue_title.tolist()\n",
    "url = df.issue_url.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue Body:\n",
      " i'm opening this issue because: - npm is crashing. - x npm is producing an incorrect install. - npm is doing something i don't understand. - other _see below for feature requests_ : what's going wrong? project works localy and on other identical servers but sometimes when i run npm i and everything seems to work after waiting to install modules i have only empty node_modules folder with hidden folder .staging in which there are a ton of modules, maybe all that i need, because there are many dependencies in package file. how can the cli team reproduce the problem? <!-- please a complete description of how to reproduce the problem. include a gist of your npm-debug.log file. if you've never used gist.github.com, start here: https://github.com/emmaramirez/how-to-submit-your-npm-debug-log --> supporting information: - npm -v prints: 3.10.10 - node -v prints: v6.10.3 - npm config get registry prints: https://registry.npmjs.org/ - windows, os x/macos, or linux?: ubuntu 16 - network issues: - geographic location where npm was run: - i use a proxy to connect to the npm registry. - i use a proxy to connect to the web. - i use a proxy when downloading git repos. - i access the npm registry via a vpn - i don't use a proxy, but have limited or unreliable internet access. - container: - i develop using vagrant on windows. - x i develop using vagrant on os x or linux. linux vps, probably some kind of container but it works with 3 other servers from same provider, same hosting plan and provider is the one i use for the last decade. - i develop / deploy using docker. - i deploy to a paas triton, heroku . \n",
      "\n",
      "Original Title:\n",
      " after successful install sometimes all modules are inside node_modules/.staging not node_modules itself\n",
      "\n",
      ">>>>> Generated Title (Prediction): <<<<<\n",
      " vagrant up fails on windows\n",
      "\n",
      "\n",
      "Issue Body:\n",
      " i've just came across this vulnerability report. it might be already patched but though it would be better for you to know. https://tsecurity.de/de/174059/reverse-engineering/exploits/glpi-0.90.4-big5-asian-encoding-sql-injection/ you can close if it is already fixed in 9.1 or 9.2 regards \n",
      "\n",
      "Original Title:\n",
      " glpi 0.90.4 big5 asian encoding sql injection\n",
      "\n",
      ">>>>> Generated Title (Prediction): <<<<<\n",
      " vulnerability in\n",
      "\n",
      "\n",
      "Issue Body:\n",
      " following install instructions https://github.com/openopps/openopps-platform/blob/dev/install.md : installing manually from source, during npm install there is an error setting up the database: running db:migrate:up db:migrate task loading... /users/sallen/src/openopps/openopps-platform/config/application.js loading... /users/sallen/src/openopps/openopps-platform/config/connections.js fatal error: cannot read property 'map' of undefined @davidebest tracked down to an issue in config/cors.js function getoriginuris { var cfenv = require 'cfenv' ; var appenv = cfenv.getappenv ; var uris = appenv.app.uris; uris = uris.map function uri { return 'https://' + uri; } ; return uris.join ',' ; } uris is undefined \n",
      "\n",
      "Original Title:\n",
      " db:migrate:up fails when installing locally without cloud foundry\n",
      "\n",
      ">>>>> Generated Title (Prediction): <<<<<\n",
      " cannot read property of undefined\n",
      "\n",
      "\n",
      "Issue Body:\n",
      " version 0.9.4 nginx logs parsed with %h %^ %d:%t %^ %r %s %b do not report any bandwidth. bandwidth is always 0.0 b. this is a sample log that was parsed successfully without bandwidth : xx.xxx.xxx.xxx - - 23/jan/2017:05:11:08 +0000 get /static/bootstrap/css/bootstrap.css http/1.1 200 142288 https://example.com/extra-parameters mozilla/5.0 linux; u; android 5.1; en-us; atom 2 build/lmy47d applewebkit/534.30 khtml, like gecko version/4.0 ucbrowser/ mobile safari/534.30 - \n",
      "\n",
      "Original Title:\n",
      " does not report bandwidth\n",
      "\n",
      ">>>>> Generated Title (Prediction): <<<<<\n",
      " is not working\n",
      "\n",
      "\n",
      "Issue Body:\n",
      " hi there, thank you for opening an issue. please note that we try to keep the terraform issue tracker reserved for bug reports and feature requests. for general usage questions, please see: https://www.terraform.io/community.html. terraform version 0.9.3 affected resource s module terraform configuration files hcl module neil_test { source = git::http://my-stash/scm/foo/module.bar.git?ref=tag_name } debug output https://gist.github.com/ludwighoff/be196196a6cfb498915dd0f1ce24406c panic output n/a expected behavior if the ref https://www.terraform.io/docs/modules/sources.html ref is a tag, then an terraform get -update=true where the commit this tag this points to has changed i.e. a deliberately mutable tag e.g. snapshot or lkg should freshen. actual behavior if the ref https://www.terraform.io/docs/modules/sources.html ref is a tag, then an update to that tag i.e. a deliberately mutable tag e.g. snapshot or lkg does not get freshened. this _does_ work ok if the ref is a branch. steps to reproduce please list the steps required to reproduce the issue, for example: 1. a module with a source whose ref is a tag 2. update the tag to point to a new commit 3. terraform get -update=true important factoids not that i can think if. references n/a \n",
      "\n",
      "Original Title:\n",
      " module source ref=tag not freshening local module on a terraform get.\n",
      "\n",
      ">>>>> Generated Title (Prediction): <<<<<\n",
      " tag does not work when a tag is updated\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples_idx = np.random.randint(0, len(body_text), 5)\n",
    "for i in samples_idx:\n",
    "    create_title(body_text[i], tk_body, tk_title, encoder_model, decoder_model, title_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
